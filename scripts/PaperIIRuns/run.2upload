#!/bin/bash
#options for sbatch
#SBATCH --account=PR_carisma
#SBATCH --job-name=CT_comp # Job name
#SBATCH --time=0-24:00 # time limit (D-HH:MM)
#SBATCH --nodes=1 # number of nodes
#SBATCH --partition=cpuq
#SBATCH --nodelist=cn08
#SBATCH --array=1-1 # number of jobs to run
#SBATCH --ntasks=1                    # Number of tasks per job (single-core task)
#SBATCH --cpus-per-task=10            # Number of threads per task
#SBATCH --output ./sbatch_logs/run-%j.txt       # Standard out goes to this file
#SBATCH --error ./sbatch_logs/error-%j.txt      # Standard err goes to this file

# for calculating the amount of time the job takes
begin=`date +%s`
echo node: $HOSTNAME
echo start time: `date`
echo ...........

# setting up variables if provided
region=$1
size=$2
mode=$3
masksize=$4
split=$5
#TODO: add channels, zdim, and other hyper parameters  like batch size, learning rate, epoch of pretrian etc

# echo 'Running on' $region 'with' $size 'events' $mode 'mode' $masksize 'masksize'

# loading modules or conda
source /home/${USER}/.bash_profile # loading bash_proi
conda activate /mnt/beegfs/nragu/tsunami/env # conda environment

# running commands
cd $MLDir/scripts/PaperIIRuns

# # Directory with files to compress
# INPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/model/SR/PTHA"
# OUTPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/data/2upload/pthaSR"

# # Get a list of only *.dat and *.npy files in the directory, and process in batches of 10
# # FILE_LIST=$(ls $INPUT_DIR/*.pt $INPUT_DIR/*.npy 2> /dev/null | sed -n "$(( (${SLURM_ARRAY_TASK_ID} - 1) * 10 + 1 )),$(( ${SLURM_ARRAY_TASK_ID} * 10 ))p")
# FILE_LIST=$(ls $INPUT_DIR/*_d_* 2> /dev/null | sed -n "$(( (${SLURM_ARRAY_TASK_ID} - 1) * 10 + 1 )),$(( ${SLURM_ARRAY_TASK_ID} * 10 ))p")

# # Compress 10 files in parallel using xargs and gzip
# echo "$FILE_LIST" | xargs -P 10 -I {} bash -c 'gzip -v --fast -c "$1" > "$2/$(basename $1).gz"' _ {} $OUTPUT_DIR

# echo "Compressed 10 files in parallel for task ID: $SLURM_ARRAY_TASK_ID"


# tar -czvf depth_SR.tar.gz pred_d_961.npy pred_d_1773.npy pred_d_3669.npy pred_d_6941.npy true_d_53550.npy 
# tar -czvf depth_CT.tar.gz pred_d_892_direct.npy pred_d_1658_direct.npy pred_d_3454_direct.npy pred_d_7071_direct.npy true_d_53550.npy 


# # Define input and output directories
# INPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/model/SR/PTHA"
# OUTPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/data/2upload/pthaSR"

# # List of files to compress
# FILE_LIST=(pred_d_961.npy pred_d_1773.npy pred_d_3669.npy pred_d_6941.npy true_d_53550.npy)
# cd $INPUT_DIR

# # Compress each file in the list
# for file in ${FILE_LIST[@]}; do
#     gzip -v --fast -c "$file" > "$OUTPUT_DIR/$(basename $file).gz"
# done

# Define input and output directories
INPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/model/CT/PTHA"
OUTPUT_DIR="/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/data/2upload/pthaCT"
FILE_LIST=(pred_d_892_direct.npy pred_d_1658_direct.npy pred_d_3454_direct.npy pred_d_7071_direct.npy true_d_53550.npy)
cd $INPUT_DIR

# Compress each file in the list
for file in ${FILE_LIST[@]}; do
    gzip -v --fast -c "$file" > "$OUTPUT_DIR/$(basename $file).gz"
done


# finished commands
echo ...........
end=`date +%s`
elapsed=`expr $end - $begin`
echo Time taken: $elapsed seconds