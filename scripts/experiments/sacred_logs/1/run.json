{
  "artifacts": [],
  "command": "run_experiment",
  "experiment": {
    "base_dir": "/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/scripts/experiments",
    "dependencies": [
      "contextily==1.3.0",
      "matplotlib==3.7.1",
      "numpy==1.24.3",
      "pandas==1.5.3",
      "sacred==0.8.4",
      "scikit-learn==1.0.2",
      "scipy==1.8.1",
      "torch==2.0.1",
      "torchsummary==1.5.1",
      "xarray==2023.5.0"
    ],
    "mainfile": "experiment.py",
    "name": "autoencoder_experiment",
    "repositories": [
      {
        "commit": "269a759a35715dbb9edc651a9f4f2d301578acf4",
        "dirty": true,
        "url": "https://github.com/naveenragur/ML4SicilyTsunami.git"
      }
    ],
    "sources": [
      [
        "experiment.py",
        "_sources/experiment_87b45d6e2693b6b0e90edbc54436ea45.py"
      ]
    ]
  },
  "fail_trace": [
    "Traceback (most recent call last):\n",
    "  File \"/mnt/beegfs/nragu/tsunami/env/lib/python3.10/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n",
    "  File \"/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/scripts/experiments/preprocess.py\", line 13, in run_experiment\n    size, path = exp.sample_events(wt_para = 'gridcount', #'LocationCount', 'mean_prob', 'importance', 'uniform_wt', 'gridcount'\n",
    "  File \"/mnt/beegfs/nragu/tsunami/env/lib/python3.10/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n",
    "  File \"/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/scripts/experiments/experiment.py\", line 1008, in sample_events\n    sample_step1 = sample_train_events(data.groupby('event_type').get_group(1),\n",
    "  File \"/mnt/beegfs/nragu/tsunami/ML4SicilyTsunami/scripts/experiments/experiment.py\", line 965, in sample_train_events\n    sampled_ids = np.random.choice(events_in_bin['id'],\n",
    "  File \"mtrand.pyx\", line 984, in numpy.random.mtrand.RandomState.choice\n",
    "ValueError: Cannot take a larger sample than population when 'replace=False'\n"
  ],
  "heartbeat": "2023-07-19T13:26:28.585265",
  "host": {
    "ENV": {},
    "cpu": "AMD EPYC 7542 32-Core Processor",
    "gpus": {
      "driver_version": "525.85.12",
      "gpus": [
        {
          "model": "NVIDIA A100 80GB PCIe",
          "persistence_mode": true,
          "total_memory": 81920
        },
        {
          "model": "NVIDIA A100 80GB PCIe",
          "persistence_mode": true,
          "total_memory": 81920
        },
        {
          "model": "NVIDIA A100 80GB PCIe",
          "persistence_mode": true,
          "total_memory": 81920
        },
        {
          "model": "NVIDIA A100 80GB PCIe",
          "persistence_mode": true,
          "total_memory": 81920
        }
      ]
    },
    "hostname": "gp01",
    "os": [
      "Linux",
      "Linux-4.18.0-425.3.1.el8.x86_64-x86_64-with-glibc2.28"
    ],
    "python_version": "3.10.11"
  },
  "meta": {
    "command": "run_experiment",
    "config_updates": {},
    "named_configs": [],
    "options": {
      "--beat-interval": null,
      "--capture": null,
      "--comment": null,
      "--debug": false,
      "--enforce_clean": false,
      "--file_storage": null,
      "--force": false,
      "--help": false,
      "--id": null,
      "--loglevel": null,
      "--mongo_db": null,
      "--name": null,
      "--pdb": false,
      "--print-config": false,
      "--priority": null,
      "--queue": false,
      "--s3": null,
      "--sql": null,
      "--tiny_db": null,
      "--unobserved": false,
      "COMMAND": null,
      "UPDATE": [],
      "help": false,
      "with": false
    }
  },
  "resources": [],
  "result": null,
  "start_time": "2023-07-19T13:26:28.183688",
  "status": "FAILED",
  "stop_time": "2023-07-19T13:26:28.587404"
}